#!/usr/bin/env python3
"""
@defgroup Hairoad
Copyright (C) Seppe Sels 2024

This code is for internal use only (UAntwerpen, project members).
Bugs, bugfixes and additions to the code need to be reported to Invilab (contact: Seppe Sels).

@ingroup Hairoad

This script reads the binary and text files generated by the LiDAR and camera,
converts the data into a point cloud, performs meshing, and projects an image onto the mesh.
It supports per-vertex coloring (via Open3D) and texture mapping using MeshLab (via PyMeshLab).
The final results are saved as PLY files and visualized.
"""

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('TkAgg')
from mpl_toolkits.mplot3d import Axes3D
from plyfile import PlyData, PlyElement
from scipy.ndimage import distance_transform_edt
import open3d as o3d
import vtk

# Import PyMeshLab for MeshLab-based processing.
import pymeshlab as ml


class HairoadProcessor:
    """
    Class that encapsulates LiDAR and camera data processing routines.

    Functionality includes:
      - Reading and reshaping point cloud data from binary files.
      - Saving point clouds as PLY files.
      - Visualizing point clouds and meshes using VTK and Open3D.
      - Computing transformation matrices from image-to-3D correspondences.
      - Projecting image colors onto point clouds (per-vertex coloring).
      - Reconstructing meshes from a noisy point cloud via Poisson reconstruction.
      - Texture mapping a mesh using MeshLab via PyMeshLab.
      - Additional utility functions (plane detection, depth image filling, etc.)
    """

    def __init__(self, file_path=None):
        """
        Initialize the processor.

        Parameters:
            file_path (str): Base file path (without extension) for point cloud files.
        """
        self.file_path = file_path
        self.points_xyz_list = []  # List of numpy arrays (Nx3) for each frame
        self.points_rgb_list = []  # List of numpy arrays (Nx3) for each frame
        self.width = None
        self.height = None
        self.frame_count = None

    # ===============================================================
    # LiDAR / Point Cloud Reading and Saving Functions
    # ===============================================================
    def reshape_points(self, points, width, height):
        """
        Reshape a flat array of points into an (N x 3) array for the first frame.

        Parameters:
            points (np.ndarray): Flat array containing the point data.
            width (int): Image width (unused here, but kept for flexibility).
            height (int): Image height (defines the number of points per frame).

        Returns:
            tuple: (remaining_points, points_frame) where points_frame is the (N x 3) array.
        """
        number_of_points = height  # Adjust if the frame definition changes
        points_np = points[0:number_of_points * 3]
        remaining_points = points[number_of_points * 3:]
        points_frame = points_np.reshape(-1, 3)
        return remaining_points, points_frame

    def read_point_cloud_from_files(self):
        """
        Reads LiDAR point and color data from binary files and metadata from a text file.

        The file names are constructed from self.file_path:
          - Points: <file_path>.bin
          - Colors: <file_path>_colors.bin
          - Metadata: <file_path>.bin.txt

        Populates self.points_xyz_list and self.points_rgb_list.

        Returns:
            tuple: (points_xyz_list, points_rgb_list)
        """
        if self.file_path is None:
            raise ValueError("File path is not set.")

        points_filename = self.file_path + ".bin"
        colors_filename = self.file_path + "_colors.bin"
        textfile_name = self.file_path + ".bin.txt"

        # Read binary files
        points_np = np.fromfile(points_filename, dtype=np.float64)
        colors_np = np.fromfile(colors_filename, dtype=np.float64)

        with open(textfile_name, "r") as textfile:
            for line in textfile:
                stripped_line = line.strip()
                if stripped_line.startswith("width"):
                    self.width = int(stripped_line.split()[1])
                    self.height = int(stripped_line.split()[3])
                    points_np, points_xyz = self.reshape_points(points_np, self.width, self.height)
                    colors_np, points_rgb = self.reshape_points(colors_np, self.width, self.height)
                    self.points_xyz_list.append(points_xyz)
                    self.points_rgb_list.append(points_rgb)
                elif stripped_line.startswith("numberOfFrames"):
                    self.frame_count = int(stripped_line.split()[1])

        # Normalize colors by copying the green channel to all channels.
        for i in range(len(self.points_rgb_list)):
            green_channel = self.points_rgb_list[i][:, 1] * 255
            green_channel = 255 * (green_channel - green_channel.min()) / (green_channel.max() - green_channel.min())
            self.points_rgb_list[i][:, 0] = green_channel.astype(np.uint8)
            self.points_rgb_list[i][:, 1] = green_channel.astype(np.uint8)
            self.points_rgb_list[i][:, 2] = green_channel.astype(np.uint8)

        return self.points_xyz_list, self.points_rgb_list

    def save_as_ply(self, points, colors, output_file):
        """
        Saves a point cloud with colors to a PLY file.

        Parameters:
            points (np.ndarray): Array of shape (N, 3) containing point coordinates.
            colors (np.ndarray): Array of shape (N, 3) containing RGB colors.
            output_file (str): Output filename for the PLY file.
        """
        vertices = np.array([tuple(row) for row in points],
                            dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
        colors_arr = np.array([tuple(row) for row in colors],
                              dtype=[('red', 'uint8'), ('green', 'uint8'), ('blue', 'uint8')])

        vertices_with_colors = np.empty(len(vertices),
                                        vertices.dtype.descr + colors_arr.dtype.descr)
        for name in vertices.dtype.names:
            vertices_with_colors[name] = vertices[name]
        for name in colors_arr.dtype.names:
            vertices_with_colors[name] = colors_arr[name]

        ply_element = PlyElement.describe(vertices_with_colors, 'vertex')
        PlyData([ply_element]).write(output_file)
        print(f"Saved PLY file to {output_file}")

    # ===============================================================
    # Mesh Reconstruction and Processing Functions
    # ===============================================================
    def reconstruct_mesh_from_ply(self, ply_file, nb_neighbors=30, std_ratio=2.0,
                                  brightness_factor=5, tint_factor=1.2, poisson_depth=9):
        """
        Loads a point cloud from a PLY file, cleans it, and reconstructs a mesh using Poisson surface reconstruction.

        Parameters:
            ply_file (str): Path to the PLY file.
            nb_neighbors (int): Number of neighbors for statistical outlier removal.
            std_ratio (float): Standard deviation ratio for outlier removal.
            brightness_factor (float): Scaling factor for brightness.
            tint_factor (float): Factor to boost the red channel.
            poisson_depth (int): Depth parameter for Poisson reconstruction.

        Returns:
            o3d.geometry.TriangleMesh: The reconstructed and trimmed mesh.
        """
        pcd = o3d.io.read_point_cloud(ply_file)
        pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)
        colors = np.asarray(pcd.colors)
        colors = colors * brightness_factor
        colors[:, 0] = colors[:, 0] * tint_factor
        colors[:, 1] = colors[:, 1] * 0.8
        colors[:, 2] = colors[:, 2] * 0.8
        colors = np.clip(colors, 0, 1)
        pcd.colors = o3d.utility.Vector3dVector(colors)
        pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=100))
        mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=poisson_depth)
        densities = np.asarray(densities)
        density_threshold = np.percentile(densities, 1)
        mesh.remove_vertices_by_mask(densities < density_threshold)
        mesh.compute_vertex_normals()
        print("Mesh reconstructed from PLY and trimmed based on density.")
        return mesh

    def project_image_to_mesh(self, mesh, image, transformation_matrix, intrinsics):
        """
        Projects an image onto a mesh by assigning vertex colors based on image data.

        For each vertex in the mesh:
          - The 3D coordinate is transformed (using transformation_matrix).
          - The transformed coordinate is projected into the image plane (using intrinsics).
          - The color at the projected pixel is assigned to that vertex.

        Parameters:
            mesh (o3d.geometry.TriangleMesh): The mesh whose vertices will be colored.
            image (np.ndarray): The source image (BGR format; will be converted to RGB).
            transformation_matrix (np.ndarray): A 4x4 transformation matrix.
            intrinsics (np.ndarray): The camera intrinsics matrix.
        """
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        vertices = np.asarray(mesh.vertices)
        new_colors = []

        for v in vertices:
            v_h = np.append(v, 1.0)
            v_trans = transformation_matrix @ v_h
            v_trans = v_trans[:3]
            proj = intrinsics @ v_trans
            proj /= proj[2]
            x, y = int(proj[0]), int(proj[1])
            if 0 <= x < image_rgb.shape[1] and 0 <= y < image_rgb.shape[0]:
                color = image_rgb[y, x] / 255.0
            else:
                color = [0, 0, 0]
            new_colors.append(color)
        mesh.vertex_colors = o3d.utility.Vector3dVector(np.array(new_colors))
        print("Image projected onto mesh (per-vertex coloring). Visualizing textured mesh...")
        o3d.visualization.draw_geometries([mesh])

    def texture_map_mesh_open3d(self, mesh, image, transformation_matrix, intrinsics):
        """
        Applies texture mapping to a mesh using Open3D.

        For each vertex, its 3D coordinates are projected into the image using the provided
        transformation matrix and camera intrinsics. The projected pixel coordinates are then
        normalized (using the image dimensions) to obtain UV coordinates. These UVs are assigned
        per triangle, and the texture image is set for the mesh.

        Parameters:
            mesh (o3d.geometry.TriangleMesh): The mesh to be textured.
            image (np.ndarray): The texture image in BGR format.
            transformation_matrix (np.ndarray): The 4x4 transformation matrix.
            intrinsics (np.ndarray): The camera intrinsics matrix.
        """
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        H, W, _ = image_rgb.shape

        # Compute UV coordinates for each vertex.
        vertices = np.asarray(mesh.vertices)
        num_vertices = vertices.shape[0]
        vertex_uv = np.zeros((num_vertices, 2))
        for i, v in enumerate(vertices):
            v_h = np.hstack([v, 1.0])
            v_trans = transformation_matrix @ v_h
            v_trans = v_trans[:3]
            proj = intrinsics @ v_trans
            proj /= proj[2]
            u, v_pixel = proj[0], proj[1]
            # Normalize to [0,1] using the image dimensions.
            vertex_uv[i, 0] = u / W
            vertex_uv[i, 1] = v_pixel / H

        # Assign UV coordinates per triangle.
        triangles = np.asarray(mesh.triangles)
        n_triangles = triangles.shape[0]
        triangle_uvs = np.zeros((n_triangles * 3, 2))
        for i in range(n_triangles):
            for j in range(3):
                idx = triangles[i, j]
                triangle_uvs[i * 3 + j, :] = vertex_uv[idx, :]

        mesh.triangle_uvs = o3d.utility.Vector2dVector(triangle_uvs)
        # Open3D requires the texture as an Image (uint8).
        texture_image = o3d.geometry.Image(image_rgb.astype(np.uint8))
        mesh.textures = [texture_image]
        print("Texture mapping applied to mesh using Open3D. Visualizing textured mesh...")
        o3d.visualization.draw_geometries([mesh])

    # ===============================================================
    # MeshLab Texture Mapping via PyMeshLab
    # ===============================================================
    @staticmethod
    def texture_map_mesh_meshlab(input_mesh_file, texture_image_file, output_mesh_file):
        """
        Applies texture mapping to a mesh using MeshLab via PyMeshLab.

        This function performs the following steps:
          1. Loads the input mesh from a file.
          2. Computes UV coordinates using a planar parametrization (projecting along the Z-axis).
             (This uses the filter "compute_texcoord_parametrization" with the parameter "texparam_method" set to "planar" and "projection_axis" set to "Z".)
          3. Assigns the texture image to the mesh using the "set_mesh_texture" filter.
          4. Saves the textured mesh to the output file.

        Parameters:
            input_mesh_file (str): Path to the input mesh file (e.g., a PLY file).
            texture_image_file (str): Path to the texture image file.
            output_mesh_file (str): Path where the textured mesh will be saved.
        """
        ms = ml.MeshSet()
        ms.load_new_mesh(input_mesh_file)

        # (Optional) Uncomment the next line to print the list of available filters.
        # ms.print_filter_list()

        # Compute UV coordinates using a planar mapping (projecting along the Z axis)
        ms.apply_filter("compute_texcoord_parametrization", texparam_method="planar", projection_axis="Z")

        # Assign the texture image to the mesh
        ms.apply_filter("set_mesh_texture", texturefile=texture_image_file)

        # Save the textured mesh (including wedge texture coordinates)
        ms.save_current_mesh(output_mesh_file, save_wedge_texcoord=True)
        print(f"Textured mesh saved to {output_mesh_file} using MeshLab via PyMeshLab.")

    # ===============================================================
    # Transformation, Point Selection, and Projection Utilities
    # ===============================================================
    @staticmethod
    def save_transformation_and_intrinsics(transformation_matrix, intrinsics, filename):
        """
        Saves the transformation matrix and camera intrinsics to a text file.

        Parameters:
            transformation_matrix (np.ndarray): The 4x4 transformation matrix.
            intrinsics (np.ndarray): The camera intrinsics matrix.
            filename (str): Output text filename.
        """
        with open(filename, 'w') as f:
            f.write('Transformation Matrix:\n')
            np.savetxt(f, transformation_matrix)
            f.write('\nIntrinsics:\n')
            np.savetxt(f, intrinsics)
        print(f"Saved transformation and intrinsics to {filename}")

    @staticmethod
    def select_points_im(image):
        """
        Allows the user to select 6 points in an image via mouse clicks.

        Parameters:
            image (np.ndarray): The image on which to select points.

        Returns:
            list: A list of 6 selected (x, y) tuples.
        """
        selected_points = []

        def mouse_callback(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN and len(selected_points) < 6:
                selected_points.append((x, y))
                print(f"Point {len(selected_points)} selected: {x}, {y}")
                temp_image = image.copy()
                for i, point in enumerate(selected_points):
                    cv2.circle(temp_image, point, 5, (0, 0, 255), -1)
                    cv2.putText(temp_image, str(i + 1), (point[0] + 5, point[1] - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                cv2.imshow("Select 6 Points", temp_image)
                if len(selected_points) == 4:
                    print("4 points selected. Press any key to continue.")

        cv2.imshow("Select 6 Points", image)
        cv2.setMouseCallback("Select 6 Points", mouse_callback)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

        if len(selected_points) != 6:
            raise ValueError("You must select exactly 6 points.")
        return selected_points

    @staticmethod
    def calculate_transformation_matrix(object_points, image_points, intrinsics):
        """
        Computes the transformation matrix using solvePnP.

        Parameters:
            object_points (list or np.ndarray): List of 3D points.
            image_points (list or np.ndarray): List of corresponding 2D image points.
            intrinsics (np.ndarray): Camera intrinsics matrix.

        Returns:
            np.ndarray: The 4x4 transformation matrix.
        """
        object_points = np.array(object_points, dtype=np.float32)
        image_points = np.array(image_points, dtype=np.float32)

        success, rotation_vector, translation_vector = cv2.solvePnP(
            object_points, image_points, intrinsics, None
        )
        if not success:
            raise ValueError("solvePnP failed to compute a valid transformation.")
        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
        transformation_matrix = np.eye(4)
        transformation_matrix[:3, :3] = rotation_matrix
        transformation_matrix[:3, 3] = translation_vector.flatten()
        return transformation_matrix

    @staticmethod
    def select_points_o3d(pcd):
        """
        Uses Open3D's built-in visualizer to allow the user to select points.

        Parameters:
            pcd (o3d.geometry.PointCloud): The point cloud to display.

        Returns:
            list: The indices of the selected points.
        """
        print("\nSelect at least three correspondences using [shift + left click].")
        print("Press [shift + right click] to undo point picking.")
        print("After picking points, press 'q' (or 'a' on AZERTY keyboards) to close the window.")
        vis = o3d.visualization.VisualizerWithEditing()
        vis.create_window()
        vis.add_geometry(pcd)
        vis.get_render_option().background_color = [0.86, 0.72, 0.82]
        vis.run()  # User picks points.
        vis.destroy_window()
        return vis.get_picked_points()

    @staticmethod
    def detect_and_plot_largest_plane(XYZ):
        """
        Detects the largest plane in the point cloud using RANSAC and visualizes it.

        Parameters:
            XYZ (np.ndarray): Array of point coordinates.

        Returns:
            np.ndarray: The transformed point cloud after aligning the plane.
        """
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(XYZ)
        plane_model, inliers = pcd.segment_plane(distance_threshold=0.01,
                                                 ransac_n=3,
                                                 num_iterations=1000)
        inlier_cloud = pcd.select_by_index(inliers)
        outlier_cloud = pcd.select_by_index(inliers, invert=True)

        inlier_XYZ = np.asarray(inlier_cloud.points)
        outlier_XYZ = np.asarray(outlier_cloud.points)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(outlier_XYZ[:, 0], outlier_XYZ[:, 1], outlier_XYZ[:, 2],
                   c='b', marker='o', label='Outliers')
        ax.scatter(inlier_XYZ[:, 0], inlier_XYZ[:, 1], inlier_XYZ[:, 2],
                   c='r', marker='o', label='Plane')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        plt.legend()
        plt.show()

        [a, b, c, d] = plane_model
        normal = np.array([a, b, c])
        z_axis = np.array([0, 1, 0])
        rotation_axis = np.cross(normal, z_axis)
        rotation_angle = np.arccos(np.dot(normal, z_axis) / (np.linalg.norm(normal) * np.linalg.norm(z_axis)))
        rotation_matrix = o3d.geometry.get_rotation_matrix_from_axis_angle(rotation_axis * rotation_angle)
        pcd.rotate(rotation_matrix)
        transformed_XYZ = np.asarray(pcd.points)
        # Lazy fix for axis swapping (to be improved)
        transformed_XYZ = transformed_XYZ[:, [0, 2, 1]]
        return transformed_XYZ

    @staticmethod
    def fill_depth_image(depth_image):
        """
        Fills in missing depth values in a depth image using a distance transform.

        Parameters:
            depth_image (np.ndarray): The depth image.

        Returns:
            np.ndarray: The filled depth image.
        """
        mask = (depth_image > 0.9999) | (depth_image < 0.00001)
        distance, indices = distance_transform_edt(mask, return_indices=True)
        for i in range(depth_image.shape[0]):
            for j in range(depth_image.shape[1]):
                if depth_image[i, j] > 0.9999 or depth_image[i, j] < 0.00001:
                    ni, nj = indices[:, i, j]
                    if distance[i, j] <= 10:
                        depth_image[i, j] = depth_image[ni, nj]
                    else:
                        depth_image[i, j] = np.nan
        return depth_image

    @staticmethod
    def project_and_color_pointcloud(image, transformation_matrix, intrinsics, point_cloud):
        """
        Projects 3D points to the image plane, assigns color from the image, and visualizes
        the colored point cloud using Open3D.

        Parameters:
            image (np.ndarray): The source image (BGR format; will be converted to RGB).
            transformation_matrix (np.ndarray): 4x4 transformation matrix.
            intrinsics (np.ndarray): Camera intrinsics matrix.
            point_cloud (np.ndarray): Array of shape (N, 3) with 3D point coordinates.
        """
        colors = []
        transformed_points = []
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        for point in point_cloud:
            point_h = np.array([point[0], point[1], point[2], 1.0])
            transformed_point = transformation_matrix @ point_h
            transformed_point = transformed_point[:3]
            transformed_points.append(transformed_point)
            projected = intrinsics @ transformed_point
            projected /= projected[2]
            x, y = int(projected[0]), int(projected[1])
            if 0 <= x < image_rgb.shape[1] and 0 <= y < image_rgb.shape[0]:
                colors.append(image_rgb[y, x] / 255.0)
            else:
                colors.append([0, 0, 0])
        transformed_points = np.array(transformed_points)
        colors = np.array(colors)
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(transformed_points)
        pcd.colors = o3d.utility.Vector3dVector(colors)
        o3d.visualization.draw_geometries([pcd])


# ===============================================================
# Minimal Usage Example
# ===============================================================
if __name__ == "__main__":
    # --- File and Parameter Definitions ---
    pointcloud_file = r"\\datanasop3mech\ProjectData\4_Other\Hairoad\Experiments\November2024\2024-11-28_10-48-44\test_2024_11_28__10_48_44_698021"
    image_path = r"\\datanasop3mech\ProjectData\4_Other\Hairoad\Experiments\November2024\hairoad\converted\checker\undistorted\c.jpg"
    output_ply = 'outputlidar.ply'
    ply_for_mesh = output_ply  # You may use an existing PLY file here.

    # --- Create a Processor Instance and Read LiDAR Data ---
    processor = HairoadProcessor(pointcloud_file)
    points_xyz_list, points_rgb_list = processor.read_point_cloud_from_files()

    # Select a frame number (e.g., 239)
    frame_number = 239
    XYZ = points_xyz_list[frame_number]
    colors = points_rgb_list[frame_number]

    # Create an Open3D point cloud for interactive point selection.
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(XYZ)
    pcd.colors = o3d.utility.Vector3dVector(colors / 255.0)
    print("Select corresponding points in the point cloud using Open3D visualizer...")
    selected_indices = processor.select_points_o3d(pcd)
    selected_points = np.asarray(pcd.points)[selected_indices]

    # Load the image.
    image = cv2.imread(image_path)
    if image is None:
        raise IOError("Error: Could not load image.")

    # Select corresponding points in the image.
    print("Select corresponding points in the image...")
    selected_points_im = processor.select_points_im(image)

    # Define camera intrinsics (modify these values as appropriate).
    intrinsics = np.array([
        [1770.69, 0, 685.3],
        [0, 1765.03, 492.7],
        [0, 0, 1]
    ], dtype=np.float32)

    # Compute the transformation matrix.
    transformation_matrix = processor.calculate_transformation_matrix(selected_points, selected_points_im, intrinsics)
    print("Computed Transformation Matrix:")
    print(transformation_matrix)

    # Save transformation and intrinsics.
    HairoadProcessor.save_transformation_and_intrinsics(transformation_matrix, intrinsics,
                                                        'transformation_and_intrinsics2.txt')

    # Save the point cloud as a PLY file.
    processor.save_as_ply(XYZ, colors, output_ply)

    # Project the point cloud with image colors and visualize.
    processor.project_and_color_pointcloud(image, transformation_matrix, intrinsics, XYZ)

    # --- Mesh Reconstruction from PLY and Per-Vertex Texture Projection ---
    print("Reconstructing mesh from PLY file...")
    mesh = processor.reconstruct_mesh_from_ply(ply_for_mesh)

    # Visualize the mesh in an Open3D window.
    o3d.visualization.draw_geometries([mesh])

    # Project the image onto the mesh (per-vertex coloring) and visualize.
    processor.project_image_to_mesh(mesh, image, transformation_matrix, intrinsics)

    # Optionally, save the mesh with vertex colors.
    o3d.io.write_triangle_mesh("poisson_mesh_textured.ply", mesh)
    print("Textured mesh saved as 'poisson_mesh_textured.ply'.")

    # --- Texture Mapping Using Open3D ---
    print("Applying texture mapping to mesh using Open3D...")
    #processor.texture_map_mesh_open3d(mesh, image, transformation_matrix, intrinsics)

    # --- Alternatively: Texture Mapping Using MeshLab via PyMeshLab ---
    # (Make sure you have a texture image file available; here we reuse image_path.)
    textured_mesh_output = "poisson_mesh_textured_meshlab.ply"
    print("Applying texture mapping using MeshLab (PyMeshLab)...")
    HairoadProcessor.texture_map_mesh_meshlab(ply_for_mesh, image_path, textured_mesh_output)
